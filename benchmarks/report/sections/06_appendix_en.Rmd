# Appendix {-}

## A. Full Scenario Parameter Table {-}

```{r scenario-params, echo=FALSE}
# Define all scenario parameters
scenario_params <- data.frame(
  ID = c(
    # S1: Baseline
    "S1_n500_p50", "S1_n500_p100", "S1_n1000_p50", "S1_n1000_p100",
    "S1_n2000_p50", "S1_n2000_p100",
    # S2: Dimension sweep
    "S2_n500_p100", "S2_n500_p500", "S2_n500_p1000",
    "S2_n1000_p100", "S2_n1000_p500", "S2_n1000_p1000",
    # S3: Heterogeneous linear
    "S3_str0", "S3_str0.5", "S3_str1", "S3_str2",
    # S4: Complex heterogeneity
    "S4a_nonlinear", "S4b_subgroup", "S4c_qualitative",
    # S5: Nonlinear confounding
    "S5_quadratic", "S5_trigonometric", "S5_interaction", "S5_combined", "S5_threshold",
    # S6: Dense confounding
    "S6_conf10", "S6_conf50", "S6_conf100", "S6_conf200",
    # S7: Weak overlap
    "S7_good", "S7_moderate", "S7_weak", "S7_extreme",
    # S8: Covariate shift
    "S8_mean", "S8_variance",
    # S9: Correlated confounding
    "S9_block", "S9_ar1", "S9_factor",
    # S10: Unobserved confounding
    "S10_str0", "S10_str0.5", "S10_str1", "S10_str2",
    # S11: Collider
    "S11_str0", "S11_str0.5", "S11_str1", "S11_str2"
  ),
  DGP = c(
    rep("dgp_baseline", 6),
    rep("dgp_dimension_sweep", 6),
    rep("dgp_heterogeneous_linear", 4),
    "dgp_heterogeneous_nonlinear", "dgp_heterogeneous_subgroup", "dgp_heterogeneous_qualitative",
    rep("dgp_nonlinear_confounding", 5),
    rep("dgp_dense_confounding", 4),
    rep("dgp_weak_overlap", 4),
    rep("dgp_covariate_shift", 2),
    rep("dgp_correlated_confounding", 3),
    rep("dgp_unobserved_confounding", 4),
    rep("dgp_collider", 4)
  ),
  n = c(
    500, 500, 1000, 1000, 2000, 2000,
    500, 500, 500, 1000, 1000, 1000,
    rep(1000, 4),
    rep(1000, 3),
    rep(1000, 5),
    rep(1000, 4),
    rep(1000, 4),
    rep(1000, 2),
    rep(1000, 3),
    rep(1000, 4),
    rep(1000, 4)
  ),
  p = c(
    50, 100, 50, 100, 50, 100,
    100, 500, 1000, 100, 500, 1000,
    rep(100, 4),
    rep(100, 3),
    rep(100, 5),
    rep(500, 4),
    rep(100, 4),
    rep(100, 2),
    rep(100, 3),
    rep(100, 4),
    rep(100, 4)
  ),
  Key_Parameter = c(
    rep("-", 6),
    rep("-", 6),
    "strength=0", "strength=0.5", "strength=1", "strength=2",
    "-", "-", "-",
    "type=quadratic", "type=trigonometric", "type=interaction", "type=combined", "type=threshold",
    "n_conf=10", "n_conf=50", "n_conf=100", "n_conf=200",
    "overlap=good", "overlap=moderate", "overlap=weak", "overlap=extreme",
    "shift=mean", "shift=variance",
    "corr=block", "corr=ar1", "corr=factor",
    "strength=0", "strength=0.5", "strength=1", "strength=2",
    "strength=0", "strength=0.5", "strength=1", "strength=2"
  ),
  stringsAsFactors = FALSE
)

if (requireNamespace("kableExtra", quietly = TRUE)) {
  kableExtra::kbl(scenario_params, booktabs = TRUE, longtable = TRUE,
                  caption = "Complete scenario parameter specification",
                  col.names = c("Scenario ID", "DGP Function", "n", "p", "Key Parameter")) |>
    kableExtra::kable_styling(latex_options = c("hold_position", "repeat_header"),
                              font_size = 9) |>
    kableExtra::pack_rows("S1: Baseline", 1, 6) |>
    kableExtra::pack_rows("S2: Dimension Sweep", 7, 12) |>
    kableExtra::pack_rows("S3: Heterogeneous Linear", 13, 16) |>
    kableExtra::pack_rows("S4: Complex Heterogeneity", 17, 19) |>
    kableExtra::pack_rows("S5: Nonlinear Confounding", 20, 24) |>
    kableExtra::pack_rows("S6: Dense Confounding", 25, 28) |>
    kableExtra::pack_rows("S7: Weak Overlap", 29, 32) |>
    kableExtra::pack_rows("S8: Covariate Shift", 33, 34) |>
    kableExtra::pack_rows("S9: Correlated Confounding", 35, 37) |>
    kableExtra::pack_rows("S10: Unobserved Confounding", 38, 41) |>
    kableExtra::pack_rows("S11: Collider Bias", 42, 45)
} else {
  knitr::kable(scenario_params, caption = "Complete scenario parameter specification")
}
```

## B. Full Results Tables {-}

```{r full-results-table, echo=FALSE, eval=data_available}
if (data_available && nrow(agg_df) > 0) {
  # Select key columns for display
  display_cols <- intersect(
    c("scenario_id", "method", "rmse_ate", "mean_bias", "sd_bias", "coverage", "mean_time"),
    names(agg_df)
  )

  results_display <- agg_df[, display_cols]
  results_display <- results_display[order(results_display$scenario_id, results_display$method), ]

  if (requireNamespace("kableExtra", quietly = TRUE)) {
    kableExtra::kbl(results_display, booktabs = TRUE, longtable = TRUE, digits = 3,
                    caption = "Complete aggregated results by scenario and method",
                    row.names = FALSE) |>
      kableExtra::kable_styling(latex_options = c("hold_position", "repeat_header"),
                                font_size = 8) |>
      kableExtra::column_spec(1, width = "2cm") |>
      kableExtra::column_spec(2, width = "1.5cm")
  } else {
    knitr::kable(results_display, digits = 3,
                 caption = "Complete aggregated results by scenario and method")
  }
}
```

## C. Method Implementation Details {-}

### G-formula (gformula)

```yaml
Implementation: cfomics internal
Backend: glmnet for regularized regression
Regularization: Elastic Net (alpha = 0.5, lambda via CV)
Cross-validation folds: 10
```

### HDML (hdml)

```yaml
Implementation: cfomics internal with econml backend
Backend: Python EconML library
Base learners: Random Forest, Gradient Boosting, Elastic Net
Cross-fitting folds: 5
```

### HDPS (hdps)

```yaml
Implementation: cfomics internal
Algorithm: Schneeweiss et al. (2009)
Covariate selection: Top-k by Bross bias formula
Default k: 500 covariates
```

### BCF (bcf)

```yaml
Implementation: bcf R package
MCMC burn-in: 1000 iterations
MCMC samples: 2000 iterations
Number of trees (prognostic): 200
Number of trees (treatment): 50
```

### TMLE (tmle)

```yaml
Implementation: tmle R package
Super Learner library: SL.glm, SL.glmnet, SL.ranger, SL.xgboost
Cross-validation: 10-fold
Propensity score truncation: [0.01, 0.99]
```

## D. Computational Environment {-}

```{r session-info, echo=FALSE}
cat("**R Session Information**\n\n")
cat(sprintf("R Version: %s\n", R.version.string))
cat(sprintf("Platform: %s\n", R.version$platform))
cat(sprintf("Running under: %s\n", sessionInfo()$running))
cat(sprintf("Report generated: %s\n", format(Sys.time(), "%Y-%m-%d %H:%M:%S %Z")))
cat("\n")
```

### Package Versions {-}

```{r package-versions, echo=FALSE}
# Core packages used in benchmark
core_packages <- c(
  "cfomics", "ggplot2", "knitr", "kableExtra", "patchwork",
  "glmnet", "grf", "bcf", "tmle", "SuperLearner",
  "reticulate"
)

# Check which packages are installed
installed_pkgs <- installed.packages()[, "Package"]
available_pkgs <- intersect(core_packages, installed_pkgs)

if (length(available_pkgs) > 0) {
  pkg_versions <- sapply(available_pkgs, function(pkg) {
    tryCatch(
      as.character(packageVersion(pkg)),
      error = function(e) "not installed"
    )
  })

  pkg_df <- data.frame(
    Package = names(pkg_versions),
    Version = pkg_versions,
    row.names = NULL,
    stringsAsFactors = FALSE
  )

  if (requireNamespace("kableExtra", quietly = TRUE)) {
    kableExtra::kbl(pkg_df, booktabs = TRUE,
                    caption = "Package versions") |>
      kableExtra::kable_styling(latex_options = c("hold_position"))
  } else {
    knitr::kable(pkg_df, caption = "Package versions")
  }
} else {
  cat("Package version information not available.\n")
}
```

## E. Reproducing the Benchmark {-}

To reproduce the benchmark results:

```r
# 1. Install cfomics and dependencies
# devtools::install_github("ymatts/cfomics")

# 2. Set up Python environment (for HDML, TMLE with Python backends)
# cfomics::cf_install_python_env("unified")
# cfomics::cf_use_python_env("unified")

# 3. Run the full benchmark (warning: may take several hours)
setwd("path/to/cfomics")
source("benchmarks/run_full_benchmark.R")

# 4. Generate the report
rmarkdown::render(
 "benchmarks/report/benchmark_report.Rmd",
  params = list(
    lang = "en",
    results_dir = "benchmarks/results"
  ),
  output_file = "benchmark_report.pdf"
)
```

### Quick Test Run {-}

For a quick test with reduced replications:

```r
# Load smoke test configuration
source("benchmarks/config.R")
cfg <- benchmark_config_smoke()  # 3 scenarios, 2 reps

# Run with reduced settings
source("benchmarks/run_full_benchmark.R")
run_benchmark(config = cfg)
```

## F. Statistical Test Assumptions {-}

### Friedman Test

The Friedman test used in this report assumes:

1. **Block design**: Each scenario represents a block, with methods compared within blocks
2. **Independence**: Scenarios are independent (different random seeds)
3. **Ordinal responses**: RMSE can be meaningfully ranked within each scenario

### Nemenyi Post-hoc Test

The Nemenyi test provides pairwise comparisons while controlling the family-wise error rate. Two methods are significantly different if their mean rank difference exceeds the Critical Difference (CD):

$$CD = q_\alpha \sqrt{\frac{k(k+1)}{6N}}$$

where $k$ is the number of methods, $N$ is the number of scenarios, and $q_\alpha$ is the Studentized range critical value.

## G. Data Availability {-}

Benchmark results are stored in:

```
benchmarks/results/
├── raw/                    # Individual replication results
│   ├── S1_n500_p50_*.rds
│   └── ...
├── summary/                # Aggregated results
│   ├── raw_results.csv     # All replications, all scenarios
│   └── aggregated_summary.csv  # Summary statistics
└── logs/                   # Execution logs
```

Raw result files (`.rds`) contain full model fits and can be used for additional analyses beyond those presented in this report.

## H. References {-}

1. Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. *The Econometrics Journal*, 21(1), C1-C68.

2. Hahn, P. R., Murray, J. S., & Carvalho, C. M. (2020). Bayesian regression tree models for causal inference: Regularization, confounding, and heterogeneous effects. *Bayesian Analysis*, 15(3), 965-1056.

3. Schneeweiss, S., Rassen, J. A., Glynn, R. J., Avorn, J., Mogun, H., & Brookhart, M. A. (2009). High-dimensional propensity score adjustment in studies of treatment effects using health care claims data. *Epidemiology*, 20(4), 512-522.

4. van der Laan, M. J., & Rose, S. (2011). *Targeted learning: Causal inference for observational and experimental data*. Springer.

5. Demsar, J. (2006). Statistical comparisons of classifiers over multiple data sets. *Journal of Machine Learning Research*, 7, 1-30.
